#!/usr/bin/env bash
#SBATCH --job-name=pttools-tests-conda
# Account configuration is required on CSC clusters
# #SBATCH --account=YOUR_GROUP_HERE
#SBATCH --partition=test
#SBATCH --time=00:10:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=1G
#SBATCH --output=pttools-tests-python.out
set -e

echo "Running on $(hostname --fqdn)"

PTTOOLS_DIR="$(realpath "..")"
cd "${PTTOOLS_DIR}"
echo "PTtools is located at: ${PTTOOLS_DIR}"

if command -v "module" &> /dev/null; then
  if [[ "$(hostname --fqdn)" == *grid.helsinki.fi ]]; then
    # For the Kale cluster of the University of Helsinki
    # https://wiki.helsinki.fi/display/it4sci/Kale+User+Guide
    module load Python/3.8.2-GCCcore-9.3.0
  else
    # For CSC clusters
    # https://docs.csc.fi/apps/python/
    # module load fgci-common
    module load python-env
    # Fix for over-eager Anaconda package configuration
    unset PYTHONPATH
  fi
else
  echo "Warning! Module management system was not found."
fi

VENV_DIR="${PTTOOLS_DIR}/venv"
if [ ! -d "${VENV_DIR}" ]; then
  echo "Creating virtualenv at: ${VENV_DIR}"
  python3 -m venv --upgrade-deps "${VENV_DIR}"
else
  echo "Using virtualenv at: ${VENV_DIR}"
fi
source "${VENV_DIR}/bin/activate"

# Override pip cache directory for Kale to avoid hitting the disk quota limits
if [ -z "${WRKDIR}" ]; then
  PIP_CACHE_DIR="$(pip cache dir)"
else
  PIP_CACHE_DIR="${WRKDIR}/cache/pip"
fi
echo "Using pip cache at: ${PIP_CACHE_DIR}"

pip --cache-dir="${PIP_CACHE_DIR}" install wheel
pip --cache-dir="${PIP_CACHE_DIR}" install -r "${PTTOOLS_DIR}/requirements.txt" -r "${PTTOOLS_DIR}/requirements-dev.txt"

numba --sysinfo

# Use as many CPU cores as requested with Slurm
NUM_CPUS="$(python -c "import os; print(len(os.sched_getaffinity(0)))")"

# You can replace this with your own workload
if command -v "srun" &> /dev/null; then
  srun pytest --numprocesses="${NUM_CPUS}"
else
  echo "Warning! Slurm was not found. Running directly."
  pytest --numprocesses="${NUM_CPUS}"
fi
