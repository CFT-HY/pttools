#!/usr/bin/env bash
#SBATCH --job-name=pttools-tests-conda
# Account configuration is required on CSC clusters
# #SBATCH --account=YOUR_GROUP_HERE
#SBATCH --partition=test
#SBATCH --time=00:10:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=1G
#SBATCH --output=pttools-tests-conda.out
set -e

echo "Running on $(hostname --fqdn)"

# Slurm scripts are placed in a special directory when executed, so this doesn't work
# SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
# PROJECT_DIR="${SCRIPT_DIR}/.."

PROJECT_DIR="$(realpath "..")"
cd "${PROJECT_DIR}"

if command -v "module" &> /dev/null; then
  if [[ "$(hostname --fqdn)" == *grid.helsinki.fi ]]; then
    # For the Kale cluster of the University of Helsinki
    # https://wiki.helsinki.fi/display/it4sci/Kale+User+Guide
    module load Anaconda3/2020.02
  else
    # For CSC clusters
    # module load fgci-common
    module load python-env
  fi
else
  echo "Warning! Module management system was not found."
fi

# Enabling conda for use in scripts
eval "$(conda shell.bash hook)"

# Conda environment setup
if [ ! -d "${PROJECT_DIR}/envs" ]; then
  # NOTE! By default Conda may download packages to ~/.conda, which may not have enough disk quota.
  # To change this behaviour, create a .condarc config file.
  # https://conda.io/projects/conda/en/latest/user-guide/configuration/use-condarc.html#
  conda env create --prefix "${PROJECT_DIR}/envs" --file environment.yml
else
  conda env update --prefix "${PROJECT_DIR}/envs" --file environment.yml
fi
conda activate "${PROJECT_DIR}/envs"

# Override pip cache directory for Kale to avoid hitting the disk quota limits
if [ -z "${WRKDIR}" ]; then
  PIP_CACHE_DIR="$(pip cache dir)"
else
  PIP_CACHE_DIR="${WRKDIR}/cache/pip"
fi

pip --cache-dir="${PIP_CACHE_DIR}" install -r "${PROJECT_DIR}/requirements-dev.txt"

numba --sysinfo

# Use as many CPU cores as requested with Slurm
NUM_CPUS="$(python -c "import os; print(len(os.sched_getaffinity(0)))")"

# You can replace this with your own workload
if command -v "srun" &> /dev/null; then
  srun pytest --numprocesses="${NUM_CPUS}"
else
  echo "Warning! Slurm was not found. Running directly."
  pytest --numprocesses="${NUM_CPUS}"
fi
